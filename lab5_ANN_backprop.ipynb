{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "lab5_ANN_backprop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banunkers/d7041e-labs/blob/master/lab5_ANN_backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G5vjADMmdmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TconNI71mdmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#functions of non-linear activations\n",
        "def f_sigmoid(X, deriv=False):\n",
        "    if not deriv:\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "    else:\n",
        "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
        "\n",
        "\n",
        "def f_softmax(X):\n",
        "    Z = np.sum(np.exp(X), axis=1)\n",
        "    Z = Z.reshape(Z.shape[0], 1)\n",
        "    return np.exp(X) / Z\n",
        "\n",
        "def f_relu(x, deriv=False):\n",
        "    if not deriv:\n",
        "        #return np.where(x > 0, x, 0)\n",
        "        return x * (x > 0)\n",
        "    else:\n",
        "        #return np.where(x >= 0, 1, 0)\n",
        "        return 1 * ( x >= 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMeEwomnmdmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exit_with_err(err_str):\n",
        "    print >> sys.stderr, err_str\n",
        "    sys.exit(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ALEvDJ4mdnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Functionality of a single hidden layer\n",
        "class Layer:\n",
        "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
        "                 activation=f_sigmoid):\n",
        "        self.is_input = is_input\n",
        "        self.is_output = is_output\n",
        "\n",
        "        # Z is the matrix that holds output values\n",
        "        self.Z = np.zeros((batch_size, size[0]))\n",
        "        # The activation function is an externally defined function (with a\n",
        "        # derivative) that is stored here\n",
        "        self.activation = activation\n",
        "\n",
        "        # W is the outgoing weight matrix for this layer\n",
        "        self.W = None\n",
        "        # S is the matrix that holds the inputs to this layer\n",
        "        self.S = None\n",
        "        # D is the matrix that holds the deltas for this layer\n",
        "        self.D = None\n",
        "        # Fp is the matrix that holds the derivatives of the activation function\n",
        "        self.Fp = None\n",
        "\n",
        "        if not is_input:\n",
        "            self.S = np.zeros((batch_size, size[0]))\n",
        "            self.D = np.zeros((batch_size, size[0]))\n",
        "\n",
        "        if not is_output:\n",
        "            self.W = np.random.normal(size=size, scale=1E-4)\n",
        "\n",
        "        if not is_input and not is_output:\n",
        "            self.Fp = np.zeros((size[0], batch_size))\n",
        "\n",
        "    def forward_propagate(self):\n",
        "        if self.is_input:\n",
        "            return self.Z.dot(self.W)\n",
        "\n",
        "        self.Z = self.activation(self.S)\n",
        "        if self.is_output:\n",
        "            return self.Z\n",
        "        else:\n",
        "            # For hidden layers, we add the bias values here\n",
        "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
        "            self.Fp = self.activation(self.S, deriv=True).T\n",
        "            return self.Z.dot(self.W)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTPg5y5KmdnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiLayerPerceptron:\n",
        "    def __init__(self, layer_config, batch_size=100, activation=f_sigmoid):\n",
        "        self.layers = []\n",
        "        self.num_layers = len(layer_config)\n",
        "        self.minibatch_size = batch_size\n",
        "\n",
        "        for i in range(self.num_layers-1):\n",
        "            if i == 0:\n",
        "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
        "                # Here, we add an additional unit at the input for the bias\n",
        "                # weight.\n",
        "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
        "                                         batch_size,\n",
        "                                         is_input=True))\n",
        "            else:\n",
        "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
        "                # Here we add an additional unit in the hidden layers for the\n",
        "                # bias weight.\n",
        "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
        "                                         batch_size,\n",
        "                                         activation=activation))\n",
        "\n",
        "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
        "        self.layers.append(Layer([layer_config[-1], None],\n",
        "                                 batch_size,\n",
        "                                 is_output=True,\n",
        "                                 activation=f_softmax))\n",
        "        print (\"Done!\")\n",
        "\n",
        "    def forward_propagate(self, data):\n",
        "        # We need to be sure to add bias values to the input\n",
        "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
        "\n",
        "        for i in range(self.num_layers-1):\n",
        "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
        "        return self.layers[-1].forward_propagate()\n",
        "\n",
        "    def backpropagate(self, yhat, labels):\n",
        "        \n",
        "        #exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
        "        # Calculates the error of the output (loss function)\n",
        "        \n",
        "        self.layers[-1].D = (yhat - labels).T\n",
        "\n",
        "        for i in range(self.num_layers-2, 0, -1):\n",
        "            # We do not calculate deltas for the bias values\n",
        "            W_nobias = self.layers[i].W[0:-1, :]\n",
        "            \n",
        "            #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\") \n",
        "            # Calculates the error of every node in the layer\n",
        "            \n",
        "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
        "\n",
        "    def update_weights(self, eta):\n",
        "        for i in range(0, self.num_layers-1):\n",
        "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
        "            self.layers[i].W += W_grad\n",
        "\n",
        "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
        "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
        "\n",
        "        N_train = len(train_labels)*len(train_labels[0])\n",
        "        N_test = len(test_labels)*len(test_labels[0])\n",
        "\n",
        "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
        "        for t in range(0, num_epochs):\n",
        "            out_str = \"[{0:4d}] \".format(t)\n",
        "\n",
        "            for b_data, b_labels in zip(train_data, train_labels):\n",
        "                output = self.forward_propagate(b_data)\n",
        "                self.backpropagate(output, b_labels)\n",
        "                \n",
        "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\") \n",
        "                # update weights performs gradient decent. ETA is the learning rate\n",
        "\n",
        "                self.update_weights(eta=eta)\n",
        "\n",
        "            if eval_train:\n",
        "                errs = 0\n",
        "                for b_data, b_labels in zip(train_data, train_labels):\n",
        "                    output = self.forward_propagate(b_data)\n",
        "                    yhat = np.argmax(output, axis=1)\n",
        "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
        "\n",
        "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
        "                                                           float(errs)/N_train))\n",
        "\n",
        "            if eval_test:\n",
        "                errs = 0\n",
        "                for b_data, b_labels in zip(test_data, test_labels):\n",
        "                    output = self.forward_propagate(b_data)\n",
        "                    yhat = np.argmax(output, axis=1)\n",
        "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
        "\n",
        "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
        "                                                       float(errs)/N_test)\n",
        "\n",
        "            print (out_str)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVBZtC8tmdnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def label_to_bit_vector(labels, nbits):\n",
        "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
        "    for i in range(labels.shape[0]):\n",
        "        bit_vector[i, labels[i]] = 1.0\n",
        "\n",
        "    return bit_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30zRU6Qzmdnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
        "    N = data.shape[0]\n",
        "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
        "\n",
        "    if N % batch_size != 0:\n",
        "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
        "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
        "    chunked_data = []\n",
        "    chunked_labels = []\n",
        "    idx = 0\n",
        "    while idx + batch_size <= N:\n",
        "        chunked_data.append(data[idx:idx+batch_size, :])\n",
        "        if not create_bit_vector:\n",
        "            chunked_labels.append(labels[idx:idx+batch_size])\n",
        "        else:\n",
        "            bit_vector = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
        "            chunked_labels.append(bit_vector)\n",
        "\n",
        "        idx += batch_size\n",
        "\n",
        "    return chunked_data, chunked_labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxGs_XYemdnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
        "    \n",
        "    print (\"Creating data...\")\n",
        "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
        "                                              batch_size,\n",
        "                                              create_bit_vector=True)\n",
        "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
        "                                              batch_size,\n",
        "                                              create_bit_vector=True)\n",
        "    print (\"Done!\")\n",
        "\n",
        "\n",
        "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eygdJPndmdn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUl1NydRmdoA",
        "colab_type": "code",
        "outputId": "f4b11559-30ed-46ed-cda9-206a81c39343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
        "\n",
        "Xtr = Xtr.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "Xtr = Xtr.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "Xtr /= 255\n",
        "X_test /= 255\n",
        "print(Xtr.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVVGA4b0mdoM",
        "colab_type": "code",
        "outputId": "398019b1-49fb-4483-ffb4-4f7262fb2485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# default config\n",
        "batch_size=100;\n",
        "\n",
        "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
        "\n",
        "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
        "\n",
        "mlp.evaluate(train_data, train_labels, valid_data, valid_labels, eval_train=True)\n",
        "\n",
        "print(\"Done:)\\n\")\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating data...\n",
            "Batch size 100, the number of examples 60000.\n",
            "Batch size 100, the number of examples 10000.\n",
            "Done!\n",
            "Initializing input layer with size 784.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing output layer with size 10.\n",
            "Done!\n",
            "Training for 70 epochs...\n",
            "[   0]  Training error: 0.38237 Test error: 0.37960\n",
            "[   1]  Training error: 0.07925 Test error: 0.07540\n",
            "[   2]  Training error: 0.04930 Test error: 0.05520\n",
            "[   3]  Training error: 0.04367 Test error: 0.05050\n",
            "[   4]  Training error: 0.03482 Test error: 0.04360\n",
            "[   5]  Training error: 0.03140 Test error: 0.03960\n",
            "[   6]  Training error: 0.02665 Test error: 0.03640\n",
            "[   7]  Training error: 0.02392 Test error: 0.03650\n",
            "[   8]  Training error: 0.02075 Test error: 0.03520\n",
            "[   9]  Training error: 0.01900 Test error: 0.03270\n",
            "[  10]  Training error: 0.01757 Test error: 0.03210\n",
            "[  11]  Training error: 0.02128 Test error: 0.03690\n",
            "[  12]  Training error: 0.01447 Test error: 0.03170\n",
            "[  13]  Training error: 0.01627 Test error: 0.03360\n",
            "[  14]  Training error: 0.01633 Test error: 0.03470\n",
            "[  15]  Training error: 0.01653 Test error: 0.03300\n",
            "[  16]  Training error: 0.01505 Test error: 0.03310\n",
            "[  17]  Training error: 0.01227 Test error: 0.03260\n",
            "[  18]  Training error: 0.01083 Test error: 0.03140\n",
            "[  19]  Training error: 0.00940 Test error: 0.03160\n",
            "[  20]  Training error: 0.01500 Test error: 0.03530\n",
            "[  21]  Training error: 0.01320 Test error: 0.03290\n",
            "[  22]  Training error: 0.00953 Test error: 0.03050\n",
            "[  23]  Training error: 0.00722 Test error: 0.02880\n",
            "[  24]  Training error: 0.01558 Test error: 0.03590\n",
            "[  25]  Training error: 0.00958 Test error: 0.03190\n",
            "[  26]  Training error: 0.00892 Test error: 0.03310\n",
            "[  27]  Training error: 0.00742 Test error: 0.03220\n",
            "[  28]  Training error: 0.01315 Test error: 0.03300\n",
            "[  29]  Training error: 0.00992 Test error: 0.03180\n",
            "[  30]  Training error: 0.01075 Test error: 0.03220\n",
            "[  31]  Training error: 0.00618 Test error: 0.02970\n",
            "[  32]  Training error: 0.00663 Test error: 0.02910\n",
            "[  33]  Training error: 0.00683 Test error: 0.02870\n",
            "[  34]  Training error: 0.00385 Test error: 0.02580\n",
            "[  35]  Training error: 0.00270 Test error: 0.02640\n",
            "[  36]  Training error: 0.00207 Test error: 0.02730\n",
            "[  37]  Training error: 0.00227 Test error: 0.02720\n",
            "[  38]  Training error: 0.00200 Test error: 0.02640\n",
            "[  39]  Training error: 0.00338 Test error: 0.02860\n",
            "[  40]  Training error: 0.00472 Test error: 0.02920\n",
            "[  41]  Training error: 0.00403 Test error: 0.02830\n",
            "[  42]  Training error: 0.00318 Test error: 0.02840\n",
            "[  43]  Training error: 0.00330 Test error: 0.02770\n",
            "[  44]  Training error: 0.00705 Test error: 0.03100\n",
            "[  45]  Training error: 0.00367 Test error: 0.02820\n",
            "[  46]  Training error: 0.00212 Test error: 0.02750\n",
            "[  47]  Training error: 0.00365 Test error: 0.02920\n",
            "[  48]  Training error: 0.00130 Test error: 0.02680\n",
            "[  49]  Training error: 0.00050 Test error: 0.02630\n",
            "[  50]  Training error: 0.00028 Test error: 0.02640\n",
            "[  51]  Training error: 0.00023 Test error: 0.02650\n",
            "[  52]  Training error: 0.00015 Test error: 0.02600\n",
            "[  53]  Training error: 0.00012 Test error: 0.02640\n",
            "[  54]  Training error: 0.00010 Test error: 0.02670\n",
            "[  55]  Training error: 0.00012 Test error: 0.02670\n",
            "[  56]  Training error: 0.00010 Test error: 0.02700\n",
            "[  57]  Training error: 0.00010 Test error: 0.02710\n",
            "[  58]  Training error: 0.00008 Test error: 0.02680\n",
            "[  59]  Training error: 0.00008 Test error: 0.02690\n",
            "[  60]  Training error: 0.00007 Test error: 0.02690\n",
            "[  61]  Training error: 0.00005 Test error: 0.02680\n",
            "[  62]  Training error: 0.00005 Test error: 0.02680\n",
            "[  63]  Training error: 0.00005 Test error: 0.02710\n",
            "[  64]  Training error: 0.00005 Test error: 0.02710\n",
            "[  65]  Training error: 0.00003 Test error: 0.02720\n",
            "[  66]  Training error: 0.00002 Test error: 0.02720\n",
            "[  67]  Training error: 0.00000 Test error: 0.02710\n",
            "[  68]  Training error: 0.00000 Test error: 0.02710\n",
            "[  69]  Training error: 0.00000 Test error: 0.02700\n",
            "Done:)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5DBUJpuOjx7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b238cad-567b-4311-ee65-45f9294998b3"
      },
      "source": [
        "eta = 0.005\n",
        "print(\"Learning rate = \", eta)\n",
        "\n",
        "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
        "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
        "mlp.evaluate(train_data, train_labels, valid_data, valid_labels, eval_train=True, eta=eta)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate =  0.005\n",
            "Creating data...\n",
            "Batch size 100, the number of examples 60000.\n",
            "Batch size 100, the number of examples 10000.\n",
            "Done!\n",
            "Initializing input layer with size 784.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing output layer with size 10.\n",
            "Done!\n",
            "Training for 70 epochs...\n",
            "[   0]  Training error: 0.70333 Test error: 0.70060\n",
            "[   1]  Training error: 0.64720 Test error: 0.64300\n",
            "[   2]  Training error: 0.59970 Test error: 0.59910\n",
            "[   3]  Training error: 0.45720 Test error: 0.46770\n",
            "[   4]  Training error: 0.21008 Test error: 0.20050\n",
            "[   5]  Training error: 0.11277 Test error: 0.11060\n",
            "[   6]  Training error: 0.08818 Test error: 0.08870\n",
            "[   7]  Training error: 0.07360 Test error: 0.07470\n",
            "[   8]  Training error: 0.06233 Test error: 0.06390\n",
            "[   9]  Training error: 0.05303 Test error: 0.05440\n",
            "[  10]  Training error: 0.04668 Test error: 0.04760\n",
            "[  11]  Training error: 0.04140 Test error: 0.04280\n",
            "[  12]  Training error: 0.03670 Test error: 0.03930\n",
            "[  13]  Training error: 0.03265 Test error: 0.03620\n",
            "[  14]  Training error: 0.02937 Test error: 0.03460\n",
            "[  15]  Training error: 0.02683 Test error: 0.03340\n",
            "[  16]  Training error: 0.02457 Test error: 0.03310\n",
            "[  17]  Training error: 0.02255 Test error: 0.03190\n",
            "[  18]  Training error: 0.02090 Test error: 0.03100\n",
            "[  19]  Training error: 0.01943 Test error: 0.02920\n",
            "[  20]  Training error: 0.01833 Test error: 0.02850\n",
            "[  21]  Training error: 0.01703 Test error: 0.02810\n",
            "[  22]  Training error: 0.01587 Test error: 0.02700\n",
            "[  23]  Training error: 0.01503 Test error: 0.02610\n",
            "[  24]  Training error: 0.01403 Test error: 0.02650\n",
            "[  25]  Training error: 0.01325 Test error: 0.02570\n",
            "[  26]  Training error: 0.01240 Test error: 0.02540\n",
            "[  27]  Training error: 0.01178 Test error: 0.02540\n",
            "[  28]  Training error: 0.01110 Test error: 0.02540\n",
            "[  29]  Training error: 0.01035 Test error: 0.02520\n",
            "[  30]  Training error: 0.00978 Test error: 0.02500\n",
            "[  31]  Training error: 0.00900 Test error: 0.02530\n",
            "[  32]  Training error: 0.00863 Test error: 0.02520\n",
            "[  33]  Training error: 0.00803 Test error: 0.02530\n",
            "[  34]  Training error: 0.00743 Test error: 0.02570\n",
            "[  35]  Training error: 0.00693 Test error: 0.02580\n",
            "[  36]  Training error: 0.00653 Test error: 0.02590\n",
            "[  37]  Training error: 0.00618 Test error: 0.02620\n",
            "[  38]  Training error: 0.00593 Test error: 0.02590\n",
            "[  39]  Training error: 0.00572 Test error: 0.02550\n",
            "[  40]  Training error: 0.00530 Test error: 0.02530\n",
            "[  41]  Training error: 0.00498 Test error: 0.02520\n",
            "[  42]  Training error: 0.00450 Test error: 0.02500\n",
            "[  43]  Training error: 0.00403 Test error: 0.02470\n",
            "[  44]  Training error: 0.00382 Test error: 0.02430\n",
            "[  45]  Training error: 0.00342 Test error: 0.02450\n",
            "[  46]  Training error: 0.00325 Test error: 0.02460\n",
            "[  47]  Training error: 0.00298 Test error: 0.02440\n",
            "[  48]  Training error: 0.00275 Test error: 0.02430\n",
            "[  49]  Training error: 0.00253 Test error: 0.02410\n",
            "[  50]  Training error: 0.00232 Test error: 0.02430\n",
            "[  51]  Training error: 0.00198 Test error: 0.02430\n",
            "[  52]  Training error: 0.00173 Test error: 0.02430\n",
            "[  53]  Training error: 0.00158 Test error: 0.02430\n",
            "[  54]  Training error: 0.00138 Test error: 0.02430\n",
            "[  55]  Training error: 0.00128 Test error: 0.02440\n",
            "[  56]  Training error: 0.00117 Test error: 0.02440\n",
            "[  57]  Training error: 0.00107 Test error: 0.02440\n",
            "[  58]  Training error: 0.00098 Test error: 0.02440\n",
            "[  59]  Training error: 0.00085 Test error: 0.02420\n",
            "[  60]  Training error: 0.00080 Test error: 0.02410\n",
            "[  61]  Training error: 0.00072 Test error: 0.02420\n",
            "[  62]  Training error: 0.00068 Test error: 0.02410\n",
            "[  63]  Training error: 0.00058 Test error: 0.02410\n",
            "[  64]  Training error: 0.00048 Test error: 0.02410\n",
            "[  65]  Training error: 0.00043 Test error: 0.02390\n",
            "[  66]  Training error: 0.00043 Test error: 0.02390\n",
            "[  67]  Training error: 0.00040 Test error: 0.02390\n",
            "[  68]  Training error: 0.00037 Test error: 0.02380\n",
            "[  69]  Training error: 0.00033 Test error: 0.02370\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTMC0Pj8Pi9F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ead0351-2a1c-4e90-90f2-591b9fa3b5c5"
      },
      "source": [
        "eta = 0.5\n",
        "print(\"Learning rate = \", eta)\n",
        "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
        "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
        "mlp.evaluate(train_data, train_labels, valid_data, valid_labels, eval_train=True, eta=eta)\n",
        "\n",
        "print(\"Done:)\\n\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate =  0.5\n",
            "Creating data...\n",
            "Batch size 100, the number of examples 60000.\n",
            "Batch size 100, the number of examples 10000.\n",
            "Done!\n",
            "Initializing input layer with size 784.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing output layer with size 10.\n",
            "Done!\n",
            "Training for 70 epochs...\n",
            "[   0]  Training error: 0.90965 Test error: 0.91080\n",
            "[   1]  Training error: 0.90128 Test error: 0.90200\n",
            "[   2]  Training error: 0.89558 Test error: 0.89720\n",
            "[   3]  Training error: 0.90070 Test error: 0.89680\n",
            "[   4]  Training error: 0.90248 Test error: 0.90260\n",
            "[   5]  Training error: 0.90085 Test error: 0.89910\n",
            "[   6]  Training error: 0.89782 Test error: 0.89900\n",
            "[   7]  Training error: 0.90085 Test error: 0.89910\n",
            "[   8]  Training error: 0.90248 Test error: 0.90260\n",
            "[   9]  Training error: 0.89782 Test error: 0.89900\n",
            "[  10]  Training error: 0.88763 Test error: 0.88650\n",
            "[  11]  Training error: 0.90965 Test error: 0.91080\n",
            "[  12]  Training error: 0.89782 Test error: 0.89900\n",
            "[  13]  Training error: 0.90248 Test error: 0.90260\n",
            "[  14]  Training error: 0.89558 Test error: 0.89720\n",
            "[  15]  Training error: 0.90137 Test error: 0.90420\n",
            "[  16]  Training error: 0.88763 Test error: 0.88650\n",
            "[  17]  Training error: 0.90128 Test error: 0.90200\n",
            "[  18]  Training error: 0.90248 Test error: 0.90260\n",
            "[  19]  Training error: 0.90085 Test error: 0.89910\n",
            "[  20]  Training error: 0.90085 Test error: 0.89910\n",
            "[  21]  Training error: 0.90128 Test error: 0.90200\n",
            "[  22]  Training error: 0.89782 Test error: 0.89900\n",
            "[  23]  Training error: 0.90248 Test error: 0.90260\n",
            "[  24]  Training error: 0.90263 Test error: 0.90180\n",
            "[  25]  Training error: 0.90085 Test error: 0.89910\n",
            "[  26]  Training error: 0.90070 Test error: 0.89680\n",
            "[  27]  Training error: 0.88763 Test error: 0.88650\n",
            "[  28]  Training error: 0.90248 Test error: 0.90260\n",
            "[  29]  Training error: 0.90248 Test error: 0.90260\n",
            "[  30]  Training error: 0.90085 Test error: 0.89910\n",
            "[  31]  Training error: 0.90248 Test error: 0.90260\n",
            "[  32]  Training error: 0.90085 Test error: 0.89910\n",
            "[  33]  Training error: 0.88763 Test error: 0.88650\n",
            "[  34]  Training error: 0.90248 Test error: 0.90260\n",
            "[  35]  Training error: 0.90128 Test error: 0.90200\n",
            "[  36]  Training error: 0.89782 Test error: 0.89900\n",
            "[  37]  Training error: 0.90965 Test error: 0.91080\n",
            "[  38]  Training error: 0.90248 Test error: 0.90260\n",
            "[  39]  Training error: 0.89782 Test error: 0.89900\n",
            "[  40]  Training error: 0.90248 Test error: 0.90260\n",
            "[  41]  Training error: 0.90085 Test error: 0.89910\n",
            "[  42]  Training error: 0.88763 Test error: 0.88650\n",
            "[  43]  Training error: 0.90128 Test error: 0.90200\n",
            "[  44]  Training error: 0.90248 Test error: 0.90260\n",
            "[  45]  Training error: 0.88763 Test error: 0.88650\n",
            "[  46]  Training error: 0.90128 Test error: 0.90200\n",
            "[  47]  Training error: 0.89782 Test error: 0.89900\n",
            "[  48]  Training error: 0.90137 Test error: 0.90420\n",
            "[  49]  Training error: 0.90128 Test error: 0.90200\n",
            "[  50]  Training error: 0.90263 Test error: 0.90180\n",
            "[  51]  Training error: 0.89782 Test error: 0.89900\n",
            "[  52]  Training error: 0.89782 Test error: 0.89900\n",
            "[  53]  Training error: 0.90248 Test error: 0.90260\n",
            "[  54]  Training error: 0.90248 Test error: 0.90260\n",
            "[  55]  Training error: 0.90128 Test error: 0.90200\n",
            "[  56]  Training error: 0.90137 Test error: 0.90420\n",
            "[  57]  Training error: 0.90248 Test error: 0.90260\n",
            "[  58]  Training error: 0.88763 Test error: 0.88650\n",
            "[  59]  Training error: 0.89558 Test error: 0.89720\n",
            "[  60]  Training error: 0.90248 Test error: 0.90260\n",
            "[  61]  Training error: 0.90263 Test error: 0.90180\n",
            "[  62]  Training error: 0.90085 Test error: 0.89910\n",
            "[  63]  Training error: 0.90070 Test error: 0.89680\n",
            "[  64]  Training error: 0.89782 Test error: 0.89900\n",
            "[  65]  Training error: 0.90128 Test error: 0.90200\n",
            "[  66]  Training error: 0.90137 Test error: 0.90420\n",
            "[  67]  Training error: 0.88763 Test error: 0.88650\n",
            "[  68]  Training error: 0.89782 Test error: 0.89900\n",
            "[  69]  Training error: 0.90128 Test error: 0.90200\n",
            "Done:)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5K_zLvMV0qD",
        "colab_type": "text"
      },
      "source": [
        "# Difference learning rate 0.5 and 0.005\n",
        "The learning rate of 0.5 is too high and causes the model to diconverge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HMGuuTkPqdc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b30ec7bd-b870-4e0e-d29a-3d697b0f6b9c"
      },
      "source": [
        "print(\"ReLU output function\")\n",
        "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
        "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size, activation=f_relu)\n",
        "mlp.evaluate(train_data, train_labels, valid_data, valid_labels, eval_train=True, eta=eta)\n",
        "\n",
        "print(\"Done:)\\n\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ReLU output function\n",
            "Creating data...\n",
            "Batch size 100, the number of examples 60000.\n",
            "Batch size 100, the number of examples 10000.\n",
            "Done!\n",
            "Initializing input layer with size 784.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing output layer with size 10.\n",
            "Done!\n",
            "Training for 70 epochs...\n",
            "[   0]  Training error: 0.89558 Test error: 0.89720\n",
            "[   1]  Training error: 0.89782 Test error: 0.89900\n",
            "[   2]  Training error: 0.90128 Test error: 0.90200\n",
            "[   3]  Training error: 0.89782 Test error: 0.89900\n",
            "[   4]  Training error: 0.90248 Test error: 0.90260\n",
            "[   5]  Training error: 0.90128 Test error: 0.90200\n",
            "[   6]  Training error: 0.89782 Test error: 0.89900\n",
            "[   7]  Training error: 0.90128 Test error: 0.90200\n",
            "[   8]  Training error: 0.89782 Test error: 0.89900\n",
            "[   9]  Training error: 0.90965 Test error: 0.91080\n",
            "[  10]  Training error: 0.90085 Test error: 0.89910\n",
            "[  11]  Training error: 0.90128 Test error: 0.90200\n",
            "[  12]  Training error: 0.88763 Test error: 0.88650\n",
            "[  13]  Training error: 0.90128 Test error: 0.90200\n",
            "[  14]  Training error: 0.89782 Test error: 0.89900\n",
            "[  15]  Training error: 0.90248 Test error: 0.90260\n",
            "[  16]  Training error: 0.90263 Test error: 0.90180\n",
            "[  17]  Training error: 0.89782 Test error: 0.89900\n",
            "[  18]  Training error: 0.90070 Test error: 0.89680\n",
            "[  19]  Training error: 0.90965 Test error: 0.91080\n",
            "[  20]  Training error: 0.90128 Test error: 0.90200\n",
            "[  21]  Training error: 0.90248 Test error: 0.90260\n",
            "[  22]  Training error: 0.89782 Test error: 0.89900\n",
            "[  23]  Training error: 0.90248 Test error: 0.90260\n",
            "[  24]  Training error: 0.89558 Test error: 0.89720\n",
            "[  25]  Training error: 0.88763 Test error: 0.88650\n",
            "[  26]  Training error: 0.90070 Test error: 0.89680\n",
            "[  27]  Training error: 0.90128 Test error: 0.90200\n",
            "[  28]  Training error: 0.90248 Test error: 0.90260\n",
            "[  29]  Training error: 0.90263 Test error: 0.90180\n",
            "[  30]  Training error: 0.90128 Test error: 0.90200\n",
            "[  31]  Training error: 0.90070 Test error: 0.89680\n",
            "[  32]  Training error: 0.90965 Test error: 0.91080\n",
            "[  33]  Training error: 0.90085 Test error: 0.89910\n",
            "[  34]  Training error: 0.88763 Test error: 0.88650\n",
            "[  35]  Training error: 0.89782 Test error: 0.89900\n",
            "[  36]  Training error: 0.90128 Test error: 0.90200\n",
            "[  37]  Training error: 0.90128 Test error: 0.90200\n",
            "[  38]  Training error: 0.90248 Test error: 0.90260\n",
            "[  39]  Training error: 0.88763 Test error: 0.88650\n",
            "[  40]  Training error: 0.90137 Test error: 0.90420\n",
            "[  41]  Training error: 0.90263 Test error: 0.90180\n",
            "[  42]  Training error: 0.89782 Test error: 0.89900\n",
            "[  43]  Training error: 0.89782 Test error: 0.89900\n",
            "[  44]  Training error: 0.88763 Test error: 0.88650\n",
            "[  45]  Training error: 0.90085 Test error: 0.89910\n",
            "[  46]  Training error: 0.90128 Test error: 0.90200\n",
            "[  47]  Training error: 0.90085 Test error: 0.89910\n",
            "[  48]  Training error: 0.90248 Test error: 0.90260\n",
            "[  49]  Training error: 0.90248 Test error: 0.90260\n",
            "[  50]  Training error: 0.88763 Test error: 0.88650\n",
            "[  51]  Training error: 0.90248 Test error: 0.90260\n",
            "[  52]  Training error: 0.88763 Test error: 0.88650\n",
            "[  53]  Training error: 0.90137 Test error: 0.90420\n",
            "[  54]  Training error: 0.90248 Test error: 0.90260\n",
            "[  55]  Training error: 0.90248 Test error: 0.90260\n",
            "[  56]  Training error: 0.90128 Test error: 0.90200\n",
            "[  57]  Training error: 0.90263 Test error: 0.90180\n",
            "[  58]  Training error: 0.90128 Test error: 0.90200\n",
            "[  59]  Training error: 0.90263 Test error: 0.90180\n",
            "[  60]  Training error: 0.90263 Test error: 0.90180\n",
            "[  61]  Training error: 0.90263 Test error: 0.90180\n",
            "[  62]  Training error: 0.90137 Test error: 0.90420\n",
            "[  63]  Training error: 0.88763 Test error: 0.88650\n",
            "[  64]  Training error: 0.90128 Test error: 0.90200\n",
            "[  65]  Training error: 0.90248 Test error: 0.90260\n",
            "[  66]  Training error: 0.90085 Test error: 0.89910\n",
            "[  67]  Training error: 0.88763 Test error: 0.88650\n",
            "[  68]  Training error: 0.90248 Test error: 0.90260\n",
            "[  69]  Training error: 0.89782 Test error: 0.89900\n",
            "Done:)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}